{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8079c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from pdb import set_trace\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Directory to save logs for TensorBoard\n",
    "log_dir = \"logs/fit/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "# Define the path where your CSV files are located\n",
    "csv_files_path = '../csv_files/*.csv'\n",
    "\n",
    "# # Uncomment this section if you need to recreate a random 1% sample\n",
    "# data_frames = []\n",
    "# for file in glob.glob(csv_files_path):\n",
    "#     df = pd.read_csv(file, header=None)  # Load without headers\n",
    "#     if df.shape[1] == 7:                 # Ensure it has exactly 7 columns\n",
    "#         data_frames.append(df)           # Append if structure is correct\n",
    "#     else:\n",
    "#         print(f\"Skipping file {file} due to unexpected number of columns: {df.shape[1]}\")\n",
    "\n",
    "# # Concatenate all valid DataFrames\n",
    "# all_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# # Assign column names\n",
    "# all_data.columns = ['x', 'y', 'z', 'p_id', 'xi1', 'xi2', 'gn']\n",
    "\n",
    "# # Drop any rows with NaN values\n",
    "# all_data.dropna(inplace=True)\n",
    "\n",
    "# # Randomly sample 1% of the data\n",
    "# sampled_data = all_data.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# # Save the sample\n",
    "# sampled_data.to_csv('sampled_data_1_percent.csv', index=False)\n",
    "\n",
    "# Load the pre-saved 1% sampled data\n",
    "sampled_data = pd.read_csv('sampled_data_1_percent.csv')\n",
    "\n",
    "# Split into features (x, y, z) and labels (p_id, xi1, xi2, gn)\n",
    "features = sampled_data[['x', 'y', 'z']].values\n",
    "labels = sampled_data[['p_id', 'xi1', 'xi2', 'gn']].values\n",
    "\n",
    "# Convert to TensorFlow dataset with (features, labels) tuples\n",
    "sampled_data = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Define the split ratio (80% train, 20% test)\n",
    "train_size = int(0.8 * len(sampled_data))\n",
    "train_data = sampled_data.take(train_size)\n",
    "test_data = sampled_data.skip(train_size)\n",
    "\n",
    "# Print dataset sizes to confirm\n",
    "print(f\"Total data size (1% sample): {len(sampled_data)}\")\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adjust labels to match the output structure of the model (4 values per patch * 96 patches)\n",
    "labels_expanded = np.zeros((len(labels), 4 * 96))\n",
    "\n",
    "# Populate the expanded labels array for each sample\n",
    "for idx, (p_id, xi1, xi2, gn) in enumerate(labels):\n",
    "    # Assuming p_id is the patch index (0 to 95)\n",
    "    patch_idx = int(p_id)\n",
    "    labels_expanded[idx, patch_idx] = 1  # One-hot encoding for p_id\n",
    "    labels_expanded[idx, 96 + patch_idx] = xi1\n",
    "    labels_expanded[idx, 192 + patch_idx] = xi2\n",
    "    labels_expanded[idx, 288 + patch_idx] = gn\n",
    "\n",
    "# Update the TensorFlow dataset with expanded labels\n",
    "sampled_data = tf.data.Dataset.from_tensor_slices((features, labels_expanded))\n",
    "\n",
    "# Define the split ratio (80% train, 20% test)\n",
    "train_size = int(0.8 * len(sampled_data))\n",
    "train_data = sampled_data.take(train_size)\n",
    "test_data = sampled_data.skip(train_size)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "\n",
    "    # Classification loss for the first 96 outputs (p_id prediction)\n",
    "    p_id_true = y_true[:, :96]  # True one-hot encoded patch IDs\n",
    "    p_id_pred = y_pred[:, :96]  # Predicted patch probabilities\n",
    "\n",
    "    # Use categorical cross-entropy for patch classification\n",
    "    classification_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.categorical_crossentropy(p_id_true, p_id_pred, from_logits=True)\n",
    "    )\n",
    "\n",
    "    # Regression targets for xi1, xi2, and gn\n",
    "    xi1_true = y_true[:, 96:192]\n",
    "    xi1_pred = y_pred[:, 96:192]\n",
    "    xi2_true = y_true[:, 192:288]\n",
    "    xi2_pred = y_pred[:, 192:288]\n",
    "    gn_true = y_true[:, 288:]\n",
    "    gn_pred = y_pred[:, 288:]\n",
    "\n",
    "    # Weighted sum product using p_id_true as a mask to select the target patch\n",
    "    xi1_loss = tf.reduce_sum(p_id_true * tf.square(xi1_true - xi1_pred), axis=1)\n",
    "    xi2_loss = tf.reduce_sum(p_id_true * tf.square(xi2_true - xi2_pred), axis=1)\n",
    "    gn_loss = tf.reduce_sum(p_id_true * tf.square(gn_true - gn_pred), axis=1)\n",
    "\n",
    "    # Average regression loss over the batch\n",
    "    regression_loss = tf.reduce_mean(xi1_loss + xi2_loss + gn_loss)\n",
    "    # Total loss: Combine classification and regression losses\n",
    "    total_loss = classification_loss + regression_loss\n",
    "    return total_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed0d29",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Verify the shapes of features and labels\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Configure training and test data for batching and shuffling\n",
    "batch_size = 32\n",
    "train_data = train_data.batch(batch_size).shuffle(1000)\n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "# Display one batch to confirm\n",
    "for batch_features, batch_labels in train_data.take(1):\n",
    "    print(\"Batch features:\", batch_features.numpy())\n",
    "    print(\"Batch labels:\", batch_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cfa63",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model architecture\n",
    "input_layer = layers.Input(shape=(3,))\n",
    "\n",
    "# Dense layers to increase dimensionality\n",
    "x = layers.Dense(128, activation='relu')(input_layer)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Reshape layer for 3D convolution\n",
    "x = layers.Reshape((8, 8, 8, 1))(x)\n",
    "\n",
    "# 3D convolutional layer to capture spatial dependencies\n",
    "x = layers.Conv3D(96, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dense layer for further feature processing\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)  # Regularization\n",
    "\n",
    "# Output layers with separate activations\n",
    "classification_output = layers.Dense(96, activation='softmax')(x)    # Classification part with softmax\n",
    "regression_output = layers.Dense(288, activation=None)(x)            # Regression part with linear activation\n",
    "\n",
    "# Concatenate both outputs to form the final output\n",
    "output = layers.Concatenate()([classification_output, regression_output])\n",
    "\n",
    "# Create the model\n",
    "model = models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['mae'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af07f42",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "epochs = 5\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, \n",
    "                    validation_data=test_data, \n",
    "                    epochs=epochs,\n",
    "                    callbacks=[tensorboard_callback])\n",
    "\n",
    "# Optional: Plotting training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of samples to display for comparison\n",
    "num_samples = 10\n",
    "\n",
    "# Take a batch from the test set\n",
    "for test_features, test_labels in test_data.take(1):\n",
    "    # Make predictions on the batch\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "    # Randomly select a few samples for comparison\n",
    "    indices = np.random.choice(range(len(test_features)), num_samples, replace=False)\n",
    "    print(\"Sample Comparisons (Predicted vs True):\\n\")\n",
    "    \n",
    "    for i in indices:\n",
    "        pred = predictions[i]\n",
    "        true = test_labels[i].numpy()\n",
    "        \n",
    "        # Classification predictions (first 96 elements)\n",
    "        pred_p_id = pred[:96]\n",
    "        true_p_id_index = np.argmax(true[:96])  # True p_id index\n",
    "\n",
    "        # Sort predictions to find the top 4 likely patches\n",
    "        top4_indices = np.argsort(pred_p_id)[-4:][::-1]\n",
    "        top4_values = pred_p_id[top4_indices]\n",
    "\n",
    "        # Display predicted and true `p_id`\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(\"Predicted p_id ->\", ', '.join([f\"{idx} ({val:.2e})\" for idx, val in zip(top4_indices, top4_values)]))\n",
    "        print(f\"True p_id      -> {true_p_id_index}\")\n",
    "\n",
    "        # Surface coordinates and gn predictions for the top 2 patches\n",
    "        print(\"Predicted (xi1, xi2) ->\", ', '.join([f\"patch {idx}: ({pred[96 + idx]:.2f}, {pred[192 + idx]:.2f})\" for idx in top4_indices[:2]]))\n",
    "        print(f\"True (xi1, xi2)      -> ({true[96 + true_p_id_index]:.2f}, {true[192 + true_p_id_index]:.2f})\")\n",
    "\n",
    "        # gn predictions for the top 2 patches\n",
    "        print(\"Predicted gn      ->\", ', '.join([f\"patch {idx}: {pred[288 + idx]:.3f}\" for idx in top4_indices[:2]]))\n",
    "        print(f\"True gn           -> {true[288 + true_p_id_index]:.3f}\")\n",
    "        print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = []\n",
    "for file in glob.glob(csv_files_path):\n",
    "    df = pd.read_csv(file, header=None)  # Load without headers\n",
    "    if df.shape[1] == 7:                 # Ensure it has exactly 7 columns\n",
    "        data_frames.append(df)           # Append if structure is correct\n",
    "    else:\n",
    "        print(f\"Skipping file {file} due to unexpected number of columns: {df.shape[1]}\")\n",
    "\n",
    "# Concatenate all valid DataFrames\n",
    "all_data = pd.concat(data_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee01364",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('sampled_data_1_percent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53052bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_features)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e432c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape\n",
    "all_data = np.array(all_data)\n",
    "print(min(all_data[:,0]),max(all_data[:,0]))\n",
    "print(min(all_data[:,1]),max(all_data[:,1]))\n",
    "print(min(all_data[:,2]),max(all_data[:,2]))\n",
    "print(min(all_data[:,3]),max(all_data[:,3]))\n",
    "print(min(all_data[:,4]),max(all_data[:,4]))\n",
    "print(min(all_data[:,5]),max(all_data[:,5]))\n",
    "print(min(all_data[:,6]),max(all_data[:,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25281931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the .keras format\n",
    "model_name = 'CLASSIF_REGRESS_128_512_CONV3D_256_20EPOCHS'\n",
    "\n",
    "model.save(model_name+'.keras')\n",
    "\n",
    "import json\n",
    "\n",
    "# Save the training history\n",
    "with open(model_name+'.json', 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "\n",
    "# To later load doing:\n",
    "# from tensorflow.keras.models import load_model\n",
    "# model = load_model('my_model.keras')\n",
    "\n",
    "# with open('training_history.json', 'r') as f:\n",
    "#     history_data = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
